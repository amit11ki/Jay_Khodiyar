{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33ad7db3-4f3a-41f3-b234-ae97a4ebe622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec3dda4b-59d1-4d9e-a1db-78863435a8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\amit\\resize_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17f7496a-78ed-4433-a3b5-8ed7e5dc4ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\amit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23f0131a-3126-4c10-b8b7-91ce3390552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords_and_lemmatize(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74de0212-f2d6-466b-a242-5f5bf3909e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your dataset is in a DataFrame called 'df' with a column 'text_description'\n",
    "df['cleaned_text'] = df['crimeaditionalinfo'].apply(clean_text)\n",
    "df['tokens'] = df['cleaned_text'].apply(tokenize_text)\n",
    "df['processed_tokens'] = df['tokens'].apply(remove_stopwords_and_lemmatize)\n",
    "\n",
    "# Join tokens back into a single string\n",
    "df['processed_text'] = df['processed_tokens'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f346d19c-9772-4e63-b631-b541a039f5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  crimeaditionalinfo  \\\n",
      "0  The issue actually started when I got this ema...   \n",
      "1  The lady in the attached videos goes by the na...   \n",
      "2  This all happened a few days after I accidenta...   \n",
      "3  i am thiyagaraj I have issue with my facebook ...   \n",
      "4  My email got hacked without any clue I didnt c...   \n",
      "\n",
      "                                      processed_text  \n",
      "0  issue actually started got email first glance ...  \n",
      "1  lady attached video go name swathi iyer social...  \n",
      "2  happened day accidentally clicked strange link...  \n",
      "3  thiyagaraj issue facebook today using facebook...  \n",
      "4  email got hacked without clue didnt click down...  \n"
     ]
    }
   ],
   "source": [
    "print(df[['crimeaditionalinfo', 'processed_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a05d9c86-e6a1-4e96-af83-c0519b5f09c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=50000)\n",
    "X_tfidf = vectorizer.fit_transform(df['processed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f965ec2c-4352-4700-b9a6-1ffabb8eec55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subcategory Accuracy: 0.85\n",
      "Subcategory F1-score: 0.82\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming df is your DataFrame with 'processed_text' and 'sub_category' columns\n",
    "X = df['processed_text']\n",
    "y_sub = df['sub_category']\n",
    "\n",
    "# Encode the subcategory labels\n",
    "le = LabelEncoder()\n",
    "y_sub_encoded = le.fit_transform(y_sub)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train_sub, y_test_sub = train_test_split(X, y_sub_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the subcategory classifier\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf_train = vectorizer.fit_transform(X_train)\n",
    "X_tfidf_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Use XGBoost for the subcategory classifier\n",
    "xgb_sub = XGBClassifier(random_state=42)\n",
    "xgb_sub.fit(X_tfidf_train, y_train_sub)\n",
    "\n",
    "# Predict subcategories for the test set\n",
    "y_pred_sub = xgb_sub.predict(X_tfidf_test)\n",
    "\n",
    "# Evaluate subcategory classification performance\n",
    "sub_accuracy = accuracy_score(y_test_sub, y_pred_sub)\n",
    "sub_f1 = f1_score(y_test_sub, y_pred_sub, average='macro')\n",
    "\n",
    "print(f\"Subcategory Accuracy: {sub_accuracy:.2f}\")\n",
    "print(f\"Subcategory F1-score: {sub_f1:.2f}\")\n",
    "\n",
    "# If you need to convert the numerical predictions back to string labels\n",
    "y_pred_sub_labels = le.inverse_transform(y_pred_sub)\n",
    "\n",
    "# Now, use the subcategory predictions to classify into the higher-level categories\n",
    "# This can be done by training a separate category-level classifier or using a rule-based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68945a49-7472-4fb5-9c6e-f8fef9fe6bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_encoder.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the subcategory classifier\n",
    "joblib.dump(xgb_sub, 'subcategory_classifier.joblib')\n",
    "\n",
    "# Save the TfidfVectorizer\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# Save the LabelEncoder\n",
    "joblib.dump(le, 'label_encoder.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b114380b-a22a-483a-8ef3-ca4c6ff14899",
   "metadata": {},
   "source": [
    "#category classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1d2a6fb-3eb7-4649-aa00-2bf5034daa9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Accuracy: 0.96\n",
      "Category F1-score: 0.96\n",
      "Predicted Subcategory: Sexually Explicit Act\n",
      "Predicted Main Category: Financial Fraud Crimes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import joblib\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Assuming 'df' is your DataFrame and it has a 'category' column\n",
    "y_category = df['category']\n",
    "\n",
    "# Encode the category labels\n",
    "le_category = LabelEncoder()\n",
    "y_category_encoded = le_category.fit_transform(y_category)\n",
    "\n",
    "# Split the data for category-level classification\n",
    "_, X_test_cat, y_train_cat, y_test_cat = train_test_split(X, y_category_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Get subcategory predictions for the entire dataset\n",
    "X_tfidf_full = vectorizer.transform(X)\n",
    "subcategory_predictions = xgb_sub.predict(X_tfidf_full)\n",
    "\n",
    "# Combine TF-IDF features with subcategory predictions\n",
    "X_combined = hstack((X_tfidf_full, subcategory_predictions.reshape(-1, 1)))\n",
    "\n",
    "# Split the combined features\n",
    "X_train_combined, X_test_combined = train_test_split(X_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the category-level classifier\n",
    "rf_category = RandomForestClassifier(random_state=42)\n",
    "rf_category.fit(X_train_combined, y_train_cat)\n",
    "\n",
    "# Predict categories for the test set\n",
    "y_pred_cat = rf_category.predict(X_test_combined)\n",
    "\n",
    "# Evaluate category classification performance\n",
    "cat_accuracy = accuracy_score(y_test_cat, y_pred_cat)\n",
    "cat_f1 = f1_score(y_test_cat, y_pred_cat, average='weighted')\n",
    "\n",
    "print(f\"Category Accuracy: {cat_accuracy:.2f}\")\n",
    "print(f\"Category F1-score: {cat_f1:.2f}\")\n",
    "\n",
    "# Save the category-level classifier\n",
    "joblib.dump(rf_category, 'category_classifier.joblib')\n",
    "\n",
    "# Save the category label encoder\n",
    "joblib.dump(le_category, 'category_label_encoder.joblib')\n",
    "\n",
    "# Function to predict both subcategory and category for new text\n",
    "def predict_categories(text):\n",
    "    # Preprocess and vectorize the text\n",
    "    X_new = vectorizer.transform([text])\n",
    "    \n",
    "    # Predict subcategory\n",
    "    subcategory_pred = xgb_sub.predict(X_new)\n",
    "    subcategory_label = le.inverse_transform(subcategory_pred)[0]\n",
    "    \n",
    "    # Combine features for category prediction\n",
    "    X_combined_new = hstack((X_new, subcategory_pred.reshape(-1, 1)))\n",
    "    \n",
    "    # Predict category\n",
    "    category_pred = rf_category.predict(X_combined_new)\n",
    "    category_label = le_category.inverse_transform(category_pred)[0]\n",
    "    \n",
    "    return subcategory_label, category_label\n",
    "\n",
    "# Test the prediction function\n",
    "test_text = \"This is a test text for prediction\"\n",
    "sub_cat, main_cat = predict_categories(test_text)\n",
    "print(f\"Predicted Subcategory: {sub_cat}\")\n",
    "print(f\"Predicted Main Category: {main_cat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8961f338-1f3a-4402-b576-84a10c0c4325",
   "metadata": {},
   "source": [
    "$ load the pretrained model and do predection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "18622a38-d663-40a0-9876-09cf1b68eb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Subcategory: RapeGang Rape RGRSexually Abusive Content\n",
      "Predicted Main Category: Women/Child Related Crime\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Load the saved models and encoders\n",
    "xgb_sub = joblib.load('subcategory_classifier.joblib')\n",
    "rf_category = joblib.load('category_classifier.joblib')\n",
    "vectorizer = joblib.load('tfidf_vectorizer.joblib')\n",
    "le = joblib.load('label_encoder.joblib')\n",
    "le_category = joblib.load('category_label_encoder.joblib')\n",
    "\n",
    "def predict_categories(text):\n",
    "    \"\"\"\n",
    "    Predict the subcategory and main category for a given text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to be classified.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[str, str]: The predicted subcategory and main category labels.\n",
    "    \"\"\"\n",
    "    # Preprocess and vectorize the text\n",
    "    X_new = vectorizer.transform([text])\n",
    "    \n",
    "    # Predict subcategory\n",
    "    subcategory_pred = xgb_sub.predict(X_new)\n",
    "    subcategory_label = le.inverse_transform(subcategory_pred)[0]\n",
    "    \n",
    "    # Combine features for category prediction\n",
    "    X_combined_new = hstack((X_new, subcategory_pred.reshape(-1, 1)))\n",
    "    \n",
    "    # Predict category\n",
    "    category_pred = rf_category.predict(X_combined_new)\n",
    "    category_label = le_category.inverse_transform(category_pred)[0]\n",
    "    \n",
    "    return subcategory_label, category_label\n",
    "\n",
    "# Example usage\n",
    "new_text = \"respected sir serious matter want inform person running involve shamefull activity using woman many place kolkata gariahat ballygunge ruby hospital behind area kasba anandapur quest mall metropolis mall area acropolismall area park circus forum mall elgin rd parkstreet maidan area esplanade garia jadavpur behalasakherbazar joka bansdroni rabindrasadanexide area haridevpur triangular park lake mall area rashbehari area tollygunge thakurpukur bbd bag dalhousie area saltlake name subhro saha amlan datta izaz ahmed anirban officially work insurance office hdfclife hindustanpark ab gariahat shopping mall st floor beside reliance trend building near gariahat outside several place involve shamefull activity last year sir person subhro saha forcefully involve woman employee ex female employee shamefull afternoon evening evening night time shamefull activity many innocent people trapped suffering dirty issue long time even lodge complain threatening blackmailing issue address place mobile whatsapp easily trace track activity need investigate last year detail please help\"\n",
    "\n",
    "subcategory, main_category = predict_categories(new_text)\n",
    "print(f\"Predicted Subcategory: {subcategory}\")\n",
    "print(f\"Predicted Main Category: {main_category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0678943-3524-429e-90bb-ad2616b89ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c274f31-7899-4e6e-b53b-78ec407b0323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
